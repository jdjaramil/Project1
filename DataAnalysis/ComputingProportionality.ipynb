{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8385529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import pysentiment2 as ps\n",
    "import nltk\n",
    "import pysentiment2 as ps\n",
    "from nltk.stem.porter import *\n",
    "import os\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0bba8539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11383, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srishti/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "<ipython-input-71-7ae84a23baca>:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  df['cik'] = df['cik'].astype(np.int).astype(str).str.pad(10, fillchar='0')\n"
     ]
    }
   ],
   "source": [
    "#Load the Returns file\n",
    "df = pd.read_csv('../DataRetrieval/url_with_returns.csv')[['reportDate','filingDate', 'cik', 'ExcessReturnsEqualWeightedSnP', 'ExcessReturnsValueWeightedSnP']]\n",
    "df['cik'] = df['cik'].astype(np.int).astype(str).str.pad(10, fillchar='0')\n",
    "# Filtering out returns on or after 2016-01-01\n",
    "df = df[df.reportDate >= '2016-01-01']\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "13fc9471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srishti/opt/anaconda3/lib/python3.8/site-packages/openpyxl/worksheet/_reader.py:312: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#Define the dictionaries\n",
    "\n",
    "LMDictionary = pd.read_excel('LM_Neg.xlsx', sheet_name = 'ND_FinTerms_Negative_v2', header = None).values.reshape(-1)\n",
    "LMDictionary = set( [w.lower().strip() for w in LMDictionary ] )\n",
    "\n",
    "HarvardDictionary = list(open('Harvard_Neg_Inf.txt', 'r'))\n",
    "HarvardDictionary = set ( [w.strip().lower() for w in HarvardDictionary ] )\n",
    "\n",
    "\n",
    "#Define the stopwords dictionary\n",
    "stop_words = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8de3e0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Harvard,LM and stopwords is 2337 4187 179\n"
     ]
    }
   ],
   "source": [
    "print('Length of Harvard,LM and stopwords is', len(LMDictionary ), len(HarvardDictionary), len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9105d52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(words):\n",
    "    lemmatized_words = [WordNetLemmatizer().lemmatize(word, 'v') for word in words]\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "50b7f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize Dictionaries and stop words\n",
    "stop_words = set(lemmatize_words(stopwords.words('english')))\n",
    "HarvardDictionary = set(lemmatize_words(HarvardDictionary))\n",
    "LMDictionary = set(lemmatize_words(LMDictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "03435ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Harvard,LM and stopwords is 2337 4187 179\n"
     ]
    }
   ],
   "source": [
    "print('Length of Harvard,LM and stopwords is', len(LMDictionary ), len(HarvardDictionary), len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "464589e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#listFileNames = os.listdir('../Forms/')\n",
    "listFileNames = os.listdir('../DataRetrieval/Forms/')\n",
    "\n",
    "# Calculates the total number of documents under consideration\n",
    "N = 0\n",
    "\n",
    "# Calculate the number of documents in which a word has appeared \n",
    "globalDict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f142c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row, fileName in enumerate(listFileNames):\n",
    "    \n",
    "    cik = fileName.split('_')[0]\n",
    "    filingDate = fileName.split('_')[1].split('.')[0]\n",
    "    #path = '../Forms/{}_{}.txt'.format(cik, filingDate)\n",
    "\n",
    "    path = '../DataRetrieval/Forms/{}_{}.txt'.format(cik, filingDate)\n",
    "    \n",
    "    f = open(path, \"r\")\n",
    "    text = f.read()\n",
    "    \n",
    "    tokensOwn = nltk.word_tokenize(text)\n",
    "    tokensOwn= list(lemmatize_words(tokensOwn))\n",
    "\n",
    "    # Filtering out the documents where the numbers of words are less than 2000\n",
    "    if len(tokensOwn) >= 2000:       \n",
    "        N+=1\n",
    "        filteredText = [w for w in tokensOwn if not w in stop_words]\n",
    "        \n",
    "        # Updated the global_dict to include count of words observed in the current document\n",
    "        #Number of documents that contains the token \"word\"\n",
    "        #Df_i\n",
    "        for word in list(set(filteredText)):\n",
    "            if not word in globalDict:\n",
    "                globalDict[word] = 1\n",
    "            else:\n",
    "                globalDict[word] += 1\n",
    "        \n",
    "    f.close()\n",
    "    \n",
    "    if (row % 1000 == 0) & (row != 0): \n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "804eb757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents under consideration: 3\n"
     ]
    }
   ],
   "source": [
    "print('Total number of documents under consideration:', N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1ada2be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of distinct words across all documents: 14614\n"
     ]
    }
   ],
   "source": [
    "print('Total number of distinct words across all documents:', len(globalDict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2d92eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCountDictionary(documentTokens):\n",
    "    '''\n",
    "    Counting occurrences of a given word in a document. EVERY WORD, not just the negative words. (TF_ij)\n",
    "    documentTokens: (list) tokens of the document\n",
    "    '''\n",
    "    \n",
    "    documentWordCountDict = {}\n",
    "    for word in list(documentTokens):\n",
    "        if not word in documentWordCountDict:\n",
    "            documentWordCountDict[word] = 1\n",
    "        else:\n",
    "            documentWordCountDict[word] += 1\n",
    "            \n",
    "    return documentWordCountDict     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8b0df42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countNegativeWords(documentTokens, setNegativeWords):\n",
    "    '''\n",
    "    Counts negative words specified in setNegativeWords given the documentTokens\n",
    "    '''\n",
    "    countNegativeWords = 0\n",
    "    for word in documentTokens:\n",
    "        if word in setNegativeWords:\n",
    "            countNegativeWords+=1\n",
    "    return countNegativeWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bfa8913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightedNegativeWords(documentTokens, setNegativeWords, wordWeights):\n",
    "    '''\n",
    "    Counts weighted (wordWeights) proportion of negative words specified in setNegativeWords given the documentTokens\n",
    "    '''\n",
    "    \n",
    "    weightedNegativeWords = 0.0\n",
    "    weightedWords = 0.0\n",
    "    for word in documentTokens:\n",
    "        weightedWords += wordWeights[word]\n",
    "        if word in setNegativeWords:\n",
    "            weightedNegativeWords += wordWeights[word]\n",
    "            \n",
    "    return weightedNegativeWords/weightedWords\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "89395fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row, fileName in enumerate(listFileNames):\n",
    "    \n",
    "    cik = fileName.split('_')[0]\n",
    "    filingDate = fileName.split('_')[1].split('.')[0]\n",
    "    #path = '../Forms/{}_{}.txt'.format(cik, filingDate)\n",
    "\n",
    "    path = '../DataRetrieval/Forms/{}_{}.txt'.format(cik, filingDate)\n",
    "    \n",
    "    f = open(path, \"r\")\n",
    "    text = f.read()\n",
    "    \n",
    "    tokensOwn = nltk.word_tokenize(text)\n",
    "    tokensOwn= list(lemmatize_words(tokensOwn))\n",
    "\n",
    "    # Filtering out the documents where the numbers of words are less than 2000\n",
    "    if len(tokensOwn) >= 2000:       \n",
    "        filteredText = [w for w in tokensOwn if not w in stop_words]\n",
    "        \n",
    "        numeratorLM = countNegativeWords(filteredText, LMDictionary)\n",
    "        propLM = numeratorLM/len(filteredText)\n",
    "        \n",
    "        numeratorHar = countNegativeWords(filteredText, HarvardDictionary)\n",
    "        propHar = numeratorHar/len(filteredText)\n",
    "        \n",
    "        df.loc[(df['cik']== cik) & (df['filingDate'] == filingDate), 'propLM'] = propLM \n",
    "        df.loc[(df['cik']== cik) & (df['filingDate'] == filingDate), 'propHar'] = propHar\n",
    "        \n",
    "        \n",
    "        wCountDictionary = wordCountDictionary(filteredText)\n",
    "        a = np.mean(list(wCountDictionary.values()))\n",
    "        \n",
    "        wordWeights = {}\n",
    "        \n",
    "        for word, word_count in wCountDictionary.items():\n",
    "            wordWeights[word] = (1 + np.log(wCountDictionary[word]))*(np.log(N/globalDict[word]))/(1 + np.log(a))\n",
    "        \n",
    "        \n",
    "        weightedLM = weightedNegativeWords(filteredText, LMDictionary, wordWeights)\n",
    "        weightedHar = weightedNegativeWords(filteredText, HarvardDictionary, wordWeights)\n",
    "        \n",
    "        df.loc[(df['cik']== cik) & (df['filingDate'] == filingDate), 'weightedLM'] = weightedLM \n",
    "        df.loc[(df['cik']== cik) & (df['filingDate'] == filingDate), 'weightedHar'] = weightedHar \n",
    "                \n",
    "    f.close()\n",
    "    \n",
    "    if (row % 1000 == 0) & (row != 0): \n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fffc7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c259c2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = df[df['propLM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad8edde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('proportions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
