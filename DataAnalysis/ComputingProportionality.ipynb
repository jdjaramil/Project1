{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8385529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import pysentiment2 as ps\n",
    "import nltk\n",
    "import pysentiment2 as ps\n",
    "from nltk.stem.porter import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bba8539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11383, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JuliJaramillo\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\JuliJaramillo\\AppData\\Local\\Temp\\ipykernel_24144\\203754680.py:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  df['cik'] = df['cik'].astype(np.int).astype(str).str.pad(10, fillchar='0')\n"
     ]
    }
   ],
   "source": [
    "#Load the Returns file\n",
    "df = pd.read_csv('../DataRetrieval/url_with_returns.csv')[['reportDate','filingDate', 'cik', 'ExcessReturnsEqualWeightedSnP', 'ExcessReturnsValueWeightedSnP']]\n",
    "# df = pd.read_csv('../DataRetrieval/url_with_returns.csv')[['reportDate','filingDate', 'cik', 'ExcessReturnsEqualWeightedSnP', 'ExcessReturnsValueWeightedSnP']]\n",
    "df['cik'] = df['cik'].astype(np.int).astype(str).str.pad(10, fillchar='0')\n",
    "# Filtering out returns on or after 2016-01-01\n",
    "df = df[df.reportDate >= '2016-01-01']\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13fc9471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JuliJaramillo\\anaconda3\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:312: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#Define the dictionaries\n",
    "\n",
    "LMDictionary = pd.read_excel('LM_Neg.xlsx', sheet_name = 'ND_FinTerms_Negative_v2', header = None).values.reshape(-1)\n",
    "LMDictionary = set( [w.lower().strip() for w in LMDictionary ] )\n",
    "\n",
    "HarvardDictionary = list(open('Harvard_Neg_Inf.txt', 'r'))\n",
    "HarvardDictionary = set ( [w.strip().lower() for w in HarvardDictionary ] )\n",
    "\n",
    "#Define the stopwords dictionary\n",
    "stop_words = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "464589e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#listFileNames = os.listdir('../Forms/')\n",
    "listFileNames = os.listdir('../DataRetrieval/Forms/')\n",
    "\n",
    "# Calculates the total number of documents under consideration\n",
    "N = 0\n",
    "# Calculate the number of times a \n",
    "globalDict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f142c8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n"
     ]
    }
   ],
   "source": [
    "for row, fileName in enumerate(listFileNames):\n",
    "    \n",
    "    cik = fileName.split('_')[0]\n",
    "    filingDate = fileName.split('_')[1].split('.')[0]\n",
    "    #path = '../Forms/{}_{}.txt'.format(cik, filingDate)\n",
    "\n",
    "    path = '../DataRetrieval/Forms/{}_{}.txt'.format(cik, filingDate)\n",
    "    \n",
    "    f = open(path, \"r\")\n",
    "    text = f.read()\n",
    "    \n",
    "    tokensOwn = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Filtering out the documents where the numbers of words are less than 2000\n",
    "    if len(tokensOwn) >= 2000:       \n",
    "        N+=1\n",
    "        filteredText = [w for w in tokensOwn if not w in stop_words]\n",
    "        \n",
    "        # Updated the global_dict to include count of words observed in the current document\n",
    "        #Number of documents that contains the token \"word\"\n",
    "        #Df_i\n",
    "        for word in list(set(filteredText)):\n",
    "            if not word in globalDict:\n",
    "                globalDict[word] = 1\n",
    "            else:\n",
    "                globalDict[word] += 1\n",
    "        \n",
    "    f.close()\n",
    "    \n",
    "    if (row % 1000 == 0) & (row != 0): \n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d92eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCountDictionary(documentTokens):\n",
    "    '''\n",
    "    Counting occurrences of a given word in a document. EVERY WORD, not just the negative words. (TF_ij)\n",
    "        documentTokens: (list) tokens of the document\n",
    "    '''\n",
    "    \n",
    "    documentWordCountDict = {}\n",
    "    for word in list(documentTokens):\n",
    "        if not word in documentWordCountDict:\n",
    "            documentWordCountDict[word] = 1\n",
    "        else:\n",
    "            documentWordCountDict[word] += 1\n",
    "            \n",
    "    return documentWordCountDict     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b0df42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countNegativeWords(documentTokens, setNegativeWords):\n",
    "    countNegativeWords = 0\n",
    "    for word in documentTokens:\n",
    "        if word in setNegativeWords:\n",
    "            countNegativeWords+=1\n",
    "    return countNegativeWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfa8913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightedNegativeWords(documentTokens, setNegativeWords, wordWeights):\n",
    "    weightedNegativeWords = 0.0\n",
    "    weightedWords = 0.0\n",
    "    \n",
    "    for word in documentTokens:\n",
    "        weightedWords += wordWeights[word]\n",
    "        if word in setNegativeWords:\n",
    "            weightedNegativeWords += wordWeights[word]\n",
    "            \n",
    "    return weightedNegativeWords/weightedWords\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89395fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n"
     ]
    }
   ],
   "source": [
    "for row, fileName in enumerate(listFileNames):\n",
    "    \n",
    "    cik = fileName.split('_')[0]\n",
    "    filingDate = fileName.split('_')[1].split('.')[0]\n",
    "    #path = '../Forms/{}_{}.txt'.format(cik, filingDate)\n",
    "\n",
    "    path = '../DataRetrieval/Forms/{}_{}.txt'.format(cik, filingDate)\n",
    "    \n",
    "    f = open(path, \"r\")\n",
    "    text = f.read()\n",
    "    \n",
    "    tokensOwn = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Filtering out the documents where the numbers of words are less than 2000\n",
    "    if len(tokensOwn) >= 2000:       \n",
    "        filteredText = [w for w in tokensOwn if not w in stop_words]\n",
    "        \n",
    "        numeratorLM = countNegativeWords(filteredText, LMDictionary)\n",
    "        propLM = numeratorLM/len(filteredText)\n",
    "        \n",
    "        numeratorHar = countNegativeWords(filteredText, HarvardDictionary)\n",
    "        propHar = numeratorHar/len(filteredText)\n",
    "        \n",
    "        df.loc[(df['cik']== cik) & (df['filingDate'] == filingDate), 'propLM'] = propLM \n",
    "        df.loc[(df['cik']== cik) & (df['filingDate'] == filingDate), 'propHar'] = propHar\n",
    "        \n",
    "        \n",
    "        wCountDictionary = wordCountDictionary(filteredText)\n",
    "        a = np.mean(list(wCountDictionary.values()))\n",
    "        \n",
    "        wordWeights = {}\n",
    "        \n",
    "        for word, word_count in wCountDictionary.items():\n",
    "            wordWeights[word] = (1 + np.log(wCountDictionary[word]))*(np.log(N/globalDict[word]))/(1 + np.log(a))\n",
    "        \n",
    "        \n",
    "        weightedLM = weightedNegativeWords(filteredText, LMDictionary, wordWeights)\n",
    "        weightedHar = weightedNegativeWords(filteredText, HarvardDictionary, wordWeights)\n",
    "        \n",
    "        df.loc[(df['cik']== cik) & (df['filingDate'] == filingDate), 'weightedLM'] = weightedLM \n",
    "        df.loc[(df['cik']== cik) & (df['filingDate'] == filingDate), 'weightedHar'] = weightedHar \n",
    "                \n",
    "    f.close()\n",
    "    \n",
    "    if (row % 1000 == 0) & (row != 0): \n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c259c2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = df[df['propLM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad8edde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('proportions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
